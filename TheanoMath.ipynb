{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow when using Theano\n",
    "1. Symbolically define mathematical functions\n",
    "  * Automatically derive gradient expressions\n",
    "2. Compile expressions into executable functions\n",
    "  * theano.function([input params], output)\n",
    "3. Execute expression\n",
    "\n",
    "## Building Symbolic Expressions\n",
    "In Theano, all algorithms are defined symbolically. It's more like writing out math than writing code. The following Theano variables are symbolic; they don't have an explicit value.\n",
    "1. Tensor\n",
    "  * Scalars 0th order tensor\n",
    "  * Vectors 1th order tensor\n",
    "  * Matrices 2th order tensor\n",
    "2. Tensors ...\n",
    "  * Reductions ?\n",
    "  * Dimshuffle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano as theano\n",
    "from theano import tensor as T\n",
    "from theano import pp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.scalar()\n",
    "y = T.scalar('dd') #Optional name to help with debugging\n",
    "z = x + y\n",
    "w = z * x\n",
    "a = T.sqrt(w)\n",
    "b = T.exp(a)\n",
    "c = a ** b\n",
    "d = T.log(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c = \\sum\\limits_{i=1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$c = \\sqrt{a^2 + b^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elemwise{add,no_inplace}.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.vector()\n",
    "y = T.vector()\n",
    "# Scalar math applied elementwise\n",
    "a = x * y\n",
    "# The cross product of x and y, a binary operation on two vectors of a 3-dimensional vector space which produces another such vector.\n",
    "b = T.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vec{z} = \\vec{x} \\cdot \\vec{y}$$\n",
    "$$\\vec{z} = \\vec{x} \\times \\vec{y}$$\n",
    "$$z = \\vec{x} \\cdot y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = T.matrix()\n",
    "Y = T.matrix()\n",
    "a = T.vector()\n",
    "# Matrix matrix product ? how in latex\n",
    "B = T.dot(x, y)\n",
    "# Matrix vector product \n",
    "c = T.dot(x, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$B_{m,p} = X_{m,n} \\times Y_{n,p}$$\n",
    "$$\\vec{c}_n = X_{m,n} \\times \\vec{a}_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling and executing expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theano.function\n",
    "To actually compute things with Theano, you define symbolic functions, which can then be called with actual values to retrieve an actual value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(3.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.scalar()\n",
    "y = T.scalar()\n",
    "\n",
    "# First ar is list of SYMBOLIC inputs\n",
    "# Second arg is SYMBOLIC output\n",
    "f = theano.function([x, y], x + y)\n",
    "\n",
    "# Call it with NUMERICAL values\n",
    "# Get a NUMERICAL output\n",
    "f(1., 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared variables are a little different - they actually do have an explicit value, which can be get/set and is shared across functions which use the variable. They're also useful\n",
    "because they have state across function calls.\n",
    "The value of a shared variable can be updated in a function by using the updates argument of theano.function.\n",
    "To modify outside a function use get_value and set_value\n",
    "Use shared variables for the values that will be changed alot. Helps with optimizing GPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#x = theano.shared(0.)\n",
    "#from theano.compat.python2x import OrderedDict\n",
    "#updates[x] = x + 1\n",
    "\n",
    "#f = theano.function([], updates=updates) # Use updates when shared variables are modified in function?\n",
    "\n",
    "#f() #updates\n",
    "#x.get_value()\n",
    "\n",
    "#x.set_value(100.)\n",
    "#f() #updates\n",
    "#x.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Symbolic Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differantiation\n",
    "tensor.grad(func, [params])\n",
    "The second argument of grad can be a list (partial derivatives) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(8.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.dscalar('x')\n",
    "y = x ** 2\n",
    "gy = T.grad(y, x)\n",
    "#pp(gy)\n",
    "\n",
    "f = theano.function([x], gy)\n",
    "f(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop: scan\n",
    "Dont get it can I use this to sum stuff up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define shared variables\n",
    "k = theano.shared(0)\n",
    "n_sym = T.iscalar(\"n_sym\")\n",
    "\n",
    "results, updates = theano.scan(lambda:{k:(k+1)}, n_steps=n_sym)\n",
    "accumulator = theano.function([n_sym], [], updates=updates, allow_input_downcast=True)\n",
    "\n",
    "k.get_value()\n",
    "accumulator(5)\n",
    "k.get_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Model: $$x y$$\n",
    "Cost function: $$x y$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random\n",
    "X = np.asarray([3,4,5,6.1,6.3,2.88,8.89,5.62,6.9,1.97,8.22,9.81,4.83,7.27,5.14,3.08])\n",
    "Y = np.asarray([0.9,1.6,1.9,2.9,1.54,1.43,3.06,2.36,2.3,1.11,2.57,3.15,1.5,2.64,2.20,1.21])\n",
    "N = X.shape[0]\n",
    "training_steps = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "m = theano.shared(rng.rand(), name = 'm')\n",
    "c = theano.shared(rng.rand(), name = 'c')\n",
    "x = T.vector('X')\n",
    "y = T.vector('y')\n",
    "\n",
    "# Construct Theano expression graph\n",
    "prediction = T.dot(x,m) + c\n",
    "cost = T.sum(T.pow(prediction-y,2))/(2*N)\n",
    "g_m, g_c = T.grad(cost,[m, c])\n",
    "\n",
    "# Compile\n",
    "train = theano.function(inputs = [x,y],\n",
    "                        outputs = cost,\n",
    "                        updates = [(m,m-learning_rate*g_m), (c,c-learning_rate*g_c)])\n",
    "test = theano.function([x], prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    val = train(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression / Softmax (Binary classification)\n",
    "\n",
    "Discriminative function\n",
    "$$-p(y = 1|x) = {1 \\over 1+exp(-w \\cdot x-b)}$$\n",
    "\n",
    "Objective function (Cross-entropy)\n",
    "$$ J = -y \\cdot log p - (1 - y)log(1 - p)$$\n",
    "Model: $$x y$$\n",
    "Cost function: $$x y$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random\n",
    "\n",
    "N = 400 # number of samples\n",
    "feats = 784 # dimensionality of features\n",
    "\n",
    "# Tuple of test data\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2)) #(X, y)\n",
    "training_steps = 10000\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "w = theano.shared(rng.randn(784), name=\"w\") #vector\n",
    "b = theano.shared(0., name=\"b\") # scalar\n",
    "#print \"Initial model: \"\n",
    "#print w.get_value(), b.get_value()\n",
    "\n",
    "# Construct Theano expression graph\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w)-b)) # probability that target = 1\n",
    "prediction = p_1 > 0.5 # the prediction threshold\n",
    "xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1) # cross-entropy loss function\n",
    "cost = xent.mean() + 0.01 * (w**2).sum() # the cost to minimize\n",
    "gw, gb = T.grad(cost, [w, b])\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "                inputs = [x,y],\n",
    "                outputs = [prediction, xent],\n",
    "                updates = [(w, w-0.1*gw), (b, b-0.1*gb)]) # or used ordereddict\n",
    "predict = theano.function(inputs = [x], outputs = prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "    \n",
    "#print 'Final model:'\n",
    "#print w.get_value(), b.get_value()\n",
    "#print 'target values for D: ', D[1]\n",
    "#print 'predictions on D: ', predict(D[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (Hidden layer(s))\n",
    "\n",
    "Discriminative function\n",
    "$$p(y=1|x) = f(w_2 \\cdot (g(w_1 \\cdot x + b_1) + b_2)$$ (f and g can be sigmoid/than functions)\n",
    "\n",
    "Objective function (Cross-entropy)\n",
    "$$ J = -y \\cdot log p - (1 - y)log(1 - p)$$\n",
    "\n",
    "Model: $$x y$$\n",
    "Cost function: $$x y$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random\n",
    "N = 400 #number of samples\n",
    "feats = 784\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 10000\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.matrix(\"x\")\n",
    "y = T.vector(\"y\")\n",
    "w_1 = theano.shared(rng.randn(784,300), name=\"w1\")\n",
    "b_1 = theano.shared(np.zeros((300,)), name=\"b1\")\n",
    "w_2 = theano.shared(rng.randn(300), name=\"w2\")\n",
    "b_2 = theano.shared(0., name=\"b2\")\n",
    "\n",
    "# Construct Theano expression graph\n",
    "p_1 = T.nnet.sigmoid(-T.dot(T.nnet.sigmoid(-T.dot(x, w_1)-b_1), w_2)-b_2) # probability target = 1\n",
    "prediction = p_1 > 0.5 # prediction threshold\n",
    "xent = -y*T.log(p_1) - (1-y)*T.log(1-p_1) # Cross-entropy loss func\n",
    "cost = xent.mean() + 0.01 * (w**2).sum() # The cost to minimize\n",
    "gw_1, gb_1, gw_2, gb_2 = T.grad(cost, [w_1, b_1, w_2, b_2])\n",
    "\n",
    "# Compile\n",
    "train = theano.function(\n",
    "                inputs = [x, y],\n",
    "                outputs = [prediction, xent],\n",
    "                updates = [(w_1, w_1-0.1*gw_1), (b_1,  b_1-0.1*gb_1), (w_2,  w_2-0.1*gw_2), (b_2, b_2-0.1*gb_2)])\n",
    "predict = theano.function(inputs = [x], outputs = prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "Use scan to implement the loop operation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
